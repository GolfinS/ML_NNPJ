import pandas as pd
import numpy as np
import pickle
import os

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error

# --- ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
file_path = "processed_data/olympics_cleaned.csv"
df = pd.read_csv(file_path)

# --- ‚úÖ ‡πÅ‡∏¢‡∏Å Features ‡πÅ‡∏•‡∏∞ Target ---
features = ["Year", "Rank"]
target = "Gold"
X = df[features]
y = df[target]

# --- ‚úÖ ‡∏ó‡∏≥ Feature Scaling ---
scaler = StandardScaler()
X = scaler.fit_transform(X)

# --- ‚úÖ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- ‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest ---
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_mae = mean_absolute_error(y_test, rf_pred)
print(f"‚úÖ Random Forest MAE: {rf_mae:.2f}")

# --- ‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost ---
xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)
xgb_mae = mean_absolute_error(y_test, xgb_pred)
print(f"‚úÖ XGBoost MAE: {xgb_mae:.2f}")

# --- ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå models ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ---
os.makedirs("models", exist_ok=True)

# --- ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ---
pickle.dump(rf_model, open("models/olympics_rf.pkl", "wb"))
pickle.dump(xgb_model, open("models/olympics_xgb.pkl", "wb"))
pickle.dump(scaler, open("models/scaler_ml.pkl", "wb"))

print("üéØ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå models/")
